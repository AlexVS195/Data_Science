# -*- coding: utf-8 -*-
"""Hw7.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Bf1ufnSbKho0OW49KES_b6RcBr15Jnnc
"""

!pip install scikit-surprise

from surprise import Dataset
from surprise import SVD, SVDpp, NMF
from surprise.model_selection import cross_validate

data = Dataset.load_builtin('ml-100k')

svd = SVD()
svdpp = SVDpp()
nmf = NMF()

# Крос-валідація для кожного алгоритму
svd_results = cross_validate(svd, data, measures=['RMSE', 'MAE'], cv=5, verbose=True)
svdpp_results = cross_validate(svdpp, data, measures=['RMSE', 'MAE'], cv=5, verbose=True)
nmf_results = cross_validate(nmf, data, measures=['RMSE', 'MAE'], cv=5, verbose=True)
print("SVD results:", svd_results)
print("SVD++ results:", svdpp_results)
print("NMF results:", nmf_results)

# Вибір оптимального алгоритму на основі RMSE
best_algorithm = None
best_rmse = float('inf')

for algo, results in zip(['SVD', 'SVD++', 'NMF'], [svd_results, svdpp_results, nmf_results]):
    mean_rmse = results['test_rmse'].mean()
    if mean_rmse < best_rmse:
        best_rmse = mean_rmse
        best_algorithm = algo

print(f"Best algorithm based on RMSE: {best_algorithm} with RMSE: {best_rmse}")

# Поекспериментуем з різними параметрами алгоритмів
svd = SVD(n_factors=100, n_epochs=20, lr_all=0.005, reg_all=0.02)
svdpp = SVDpp(n_factors=50, n_epochs=15, lr_all=0.007, reg_all=0.01)
nmf = NMF(n_factors=20, n_epochs=30, reg_pu=0.06, reg_qi=0.06)

# Крос-валідація для кожного алгоритму
svd_results = cross_validate(svd, data, measures=['RMSE', 'MAE'], cv=5, verbose=True)
svdpp_results = cross_validate(svdpp, data, measures=['RMSE', 'MAE'], cv=5, verbose=True)
nmf_results = cross_validate(nmf, data, measures=['RMSE', 'MAE'], cv=5, verbose=True)
print("SVD results:", svd_results)
print("SVD++ results:", svdpp_results)
print("NMF results:", nmf_results)

# Вибір оптимального алгоритму на основі RMSE
best_algorithm = None
best_rmse = float('inf')

for algo, results in zip(['SVD', 'SVD++', 'NMF'], [svd_results, svdpp_results, nmf_results]):
    mean_rmse = results['test_rmse'].mean()
    if mean_rmse < best_rmse:
        best_rmse = mean_rmse
        best_algorithm = algo

print(f"Best algorithm based on RMSE: {best_algorithm} with RMSE: {best_rmse}")

# Додаткове завдання з зірочкою

import numpy as np
import pandas as pd
from scipy.io import loadmat

def loadMovieList():
    """
    Reads the fixed movie list in movie_ids.txt and returns a list of movie names.
    Returns
    -------
    movieNames : list
        A list of strings, representing all movie names.
    """
    with open('/content/movie_ids.txt', encoding='ISO-8859-1') as fid:
        movies = fid.readlines()

    movieNames = []
    for movie in movies:
        parts = movie.split()
        movieNames.append(' '.join(parts[1:]).strip())
    return movieNames

movies = loadMovieList()
data = loadmat('/content/movies.mat')

ratings = data['Y']
rated = data['R']

# Перетворення масиву рейтингів у DataFrame
num_users, num_movies = ratings.shape
user_ids = np.arange(1, num_users + 1)
movie_ids = np.arange(1, num_movies + 1)

ratings_df = pd.DataFrame(ratings, index=user_ids, columns=movie_ids)
rated_df = pd.DataFrame(rated, index=user_ids, columns=movie_ids)

# Створення DataFrame з ідентифікаторами та назвами фільмів
movies_df = pd.DataFrame({'movieId': range(1, len(movies) + 1), 'movieName': movies})

# Об'єднання датасету рейтингів з DataFrame назв фільмів
merged_df = ratings_df.stack().reset_index()
merged_df.columns = ['userId', 'movieId', 'rating']
merged_df = pd.merge(merged_df, movies_df, on='movieId')

# Ініціалізація параметрів моделі
num_users = merged_df['userId'].nunique()
num_movies = merged_df['movieId'].nunique()
num_factors = 10  # Кількість факторів

P = np.random.rand(num_users, num_factors) * 0.1
Q = np.random.rand(num_movies, num_factors) * 0.1

# Функція втрат
def loss_function(P, Q, user_movie_matrix, lambda_reg=0.02):
    predicted = np.dot(P, Q.T)
    error = user_movie_matrix - predicted
    loss = np.sum(error ** 2) + lambda_reg * (np.sum(P ** 2) + np.sum(Q ** 2))
    return loss

# Розрахунок градієнтів
def compute_gradients(P, Q, user_movie_matrix, lambda_reg=0.02):
    predicted = np.dot(P, Q.T)
    error = user_movie_matrix - predicted
    P_grad = -2 * np.dot(error, Q) + 2 * lambda_reg * P
    Q_grad = -2 * np.dot(error.T, P) + 2 * lambda_reg * Q
    return P_grad, Q_grad

# Навчання моделі
learning_rate = 0.0001
num_epochs = 100

user_movie_matrix = merged_df.pivot_table(index='userId', columns='movieId', values='rating')

for epoch in range(num_epochs):
    P_grad, Q_grad = compute_gradients(P, Q, user_movie_matrix.fillna(0), lambda_reg=0.02)
    P -= learning_rate * P_grad
    Q -= learning_rate * Q_grad
    loss = loss_function(P, Q, user_movie_matrix.fillna(0), lambda_reg=0.02)
    print(f'Epoch {epoch + 1}, Loss: {loss}')

# Оцінка моделі
predicted_ratings = np.dot(P, Q.T)
predicted_ratings_df = pd.DataFrame(predicted_ratings, index=user_ids, columns=movie_ids)

# Об'єднання передбачених рейтингів з DataFrame назв фільмів
predicted_ratings_long = predicted_ratings_df.stack().reset_index()
predicted_ratings_long.columns = ['userId', 'movieId', 'predicted_rating']
predicted_ratings_long = pd.merge(predicted_ratings_long, movies_df, on='movieId')

# Виведення результатів
print(predicted_ratings_long)

# Збереження результатів у CSV-файл
predicted_ratings_long.to_csv('predicted_ratings.csv', index=False)

